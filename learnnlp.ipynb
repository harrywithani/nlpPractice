{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bigrams with and without `nltk`"
      ],
      "metadata": {
        "id": "PNdBFOvrSGxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yExV6qvlNvIk",
        "outputId": "f2e041b7-502e-403b-d7a7-e3977421c500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('first', 'rule') : 1\n",
            "('rule', 'fight') : 1\n",
            "('fight', 'club') : 2\n",
            "('club', 'dont') : 1\n",
            "('dont', 'talk') : 1\n",
            "('talk', 'fight') : 1\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "stopwords = {\"the\", \"of\", \"and\", \"is\", \"you\", \"do\", \"not\", \"about\", \"as\", \"at\", \"a\", \"on\", \"if\", \"this\", \"to\", \"your\", \"they\", \"have\"}\n",
        "\n",
        "text = \"\"\"\n",
        "The first rule of Fight Club: is: you don't not talk about Fight Club.\n",
        "\"\"\"\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "tokens = text.split()\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "bigrams = list(zip(filtered_tokens, filtered_tokens[1:]))\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "for bigram, count in bigram_counts.items():\n",
        "    print(bigram, \":\", count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"\"\"\n",
        "The first rule of Fight Club is: you do not talk about Fight Club.\n",
        "\"\"\"\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "bigrams = list(zip(filtered_tokens, filtered_tokens[1:]))\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "for bigram, count in bigram_counts.items():\n",
        "    print(bigram, \":\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPeYQztfQgip",
        "outputId": "0bceef5e-f2c2-4911-e8f7-92eb3e802f4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('first', 'rule') : 1\n",
            "('rule', 'fight') : 1\n",
            "('fight', 'club') : 2\n",
            "('club', 'talk') : 1\n",
            "('talk', 'fight') : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization, Sentence Splitting, Normalization"
      ],
      "metadata": {
        "id": "Ggv_yK1YSw9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = re.findall(r\"[A-Za-z'-]+\", text)\n",
        "\n",
        "    filtered_tokens = [token.lower() for token in tokens if len(token) >= 2]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "text = \"Can't we just use well-known NLP tools? They're awesome, aren't they?\"\n",
        "tokens = tokenize_text(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udtX6oDlRClv",
        "outputId": "51f97c08-25e4-4914-a233-8f360236cad7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"can't\", 'we', 'just', 'use', 'well-known', 'nlp', 'tools', \"they're\", 'awesome', \"aren't\", 'they']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text_simple(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # replace numbers (integers or decimals) with NUMBER\n",
        "    text = re.sub(r'\\d+(\\.\\d+)?', 'NUMBER', text)\n",
        "\n",
        "    # replace multiple spaces/tabs with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # simple lemmatization: remove 'ing' or 'ed' endings\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        if words[i].endswith('ing') and len(words[i]) > 5:\n",
        "            words[i] = words[i][:-3]\n",
        "        elif words[i].endswith('ed') and len(words[i]) > 4:\n",
        "            words[i] = words[i][:-2]\n",
        "\n",
        "    # join back into a string\n",
        "    return ' '.join(words)\n",
        "\n",
        "sample_input = \"Running and jumping 123 times is fun. I walked 45.67 miles.\"\n",
        "print(normalize_text_simple(sample_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1SvxrGP0YUj",
        "outputId": "d62904a1-748b-4e19-cdb4-25caa07ef730"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runn and jump NUMBER times is fun. i walk NUMBER miles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Sentence tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "    result = []\n",
        "    for sent in sentences:\n",
        "        # Word tokenize keeps words, hyphens, apostrophes\n",
        "        tokens = re.findall(r\"\\w[\\w'-]*\", sent)\n",
        "        normalized = normalizeTokens(tokens)\n",
        "        result.append((sent, tokens, normalized))\n",
        "    return result\n",
        "\n",
        "def normalizeTokens(tokens):\n",
        "    normalized = []\n",
        "    for t in tokens:\n",
        "        t_lower = t.lower()\n",
        "        # replace numbers\n",
        "        if re.fullmatch(r'\\d+(\\.\\d+)?', t_lower):\n",
        "            normalized.append(\"NUMBER\")\n",
        "        # remove stopwords and short words\n",
        "        elif t_lower not in stop_words and len(t_lower) >= 2:\n",
        "            # simple lemmatization\n",
        "            if t_lower.endswith('ing') and len(t_lower) > 5:\n",
        "                t_lower = t_lower[:-3]\n",
        "            elif t_lower.endswith('ed') and len(t_lower) > 4:\n",
        "                t_lower = t_lower[:-2]\n",
        "            normalized.append(t_lower)\n",
        "    return normalized\n",
        "\n",
        "\n",
        "\n",
        "sample_text = \"Dr. Brown can't run 123 miles. Isn't NLP fun? He scored 98.6 points.\"\n",
        "processed = preprocess_text(sample_text)\n",
        "\n",
        "for i, (sent, tokens, normalized) in enumerate(processed, 1):\n",
        "    print(f\"Sentence {i}: {sent}\")\n",
        "    print(f\"  Tokens: {tokens}\")\n",
        "    print(f\"  Normalized: {normalized}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivYzEM417ic",
        "outputId": "125771c2-211a-470e-ef6f-019d4915ec85"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Dr. Brown can't run 123 miles.\n",
            "  Tokens: ['Dr', 'Brown', \"can't\", 'run', '123', 'miles']\n",
            "  Normalized: ['dr', 'brown', \"can't\", 'run', 'NUMBER', 'miles']\n",
            "\n",
            "Sentence 2: Isn't NLP fun?\n",
            "  Tokens: [\"Isn't\", 'NLP', 'fun']\n",
            "  Normalized: ['nlp', 'fun']\n",
            "\n",
            "Sentence 3: He scored 98.6 points.\n",
            "  Tokens: ['He', 'scored', '98', '6', 'points']\n",
            "  Normalized: ['scor', 'NUMBER', 'NUMBER', 'points']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Dr. Smith lives in the U.S.A. He loves NLP. The value of pi is 3.14. Isn't it cool?\"\n",
        "\n",
        "abbreviations = [\"Mr.\", \"Mrs.\", \"Ms.\", \"Dr.\", \"U.S.A\", \"e.g.\", \"i.e.\", \"vs.\", \"etc.\"]\n",
        "\n",
        "for abbr in abbreviations:\n",
        "    text = text.replace(abbr, abbr.replace(\".\", \"<prd>\"))\n",
        "\n",
        "text = re.sub(r'(\\d)\\.(\\d)', r'\\1<prd>\\2', text)\n",
        "\n",
        "# mark sentence boundaries\n",
        "text = text.replace(\"! \", \"!<stop>\")\n",
        "text = text.replace(\"? \", \"?<stop>\")\n",
        "text = text.replace(\". \", \".<stop>\")\n",
        "\n",
        "# restore periods\n",
        "text = text.replace(\"<prd>\", \".\")\n",
        "\n",
        "# split into sentences and clean\n",
        "sentences = text.split(\"<stop>\")\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "\n",
        "for i, s in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iUSCDGUy4ci",
        "outputId": "b296d524-1e2c-4b1e-90f8-bc9c4cd66236"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Dr. Smith lives in the U.S.A.\n",
            "2: He loves NLP.\n",
            "3: The value of pi is 3.14.\n",
            "4: Isn't it cool?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def sentence_segmentation(text):\n",
        "    # Known abbreviations\n",
        "    abbreviations = [\n",
        "    \"u.s.a\", \"u.s.a.\", \"x.ai\", \"Sr.\", \"Srta.\", \"eth\", \"ph.d\", \"phd\", \"ai\",\n",
        "    \"Mr.\", \"Mrs.\", \"Ms.\", \"Dr.\", \"U.S.A.\", \"e.g.\", \"i.e.\", \"vs.\", \"etc.\"\n",
        "]\n",
        "\n",
        "\n",
        "    # Step 1. Protect abbreviations\n",
        "    for abbr in abbreviations:\n",
        "      text = text.replace(abbr, abbr.replace(\".\", \"<prd>\"))\n",
        "\n",
        "\n",
        "    # Step 3. Protect decimals and websites\n",
        "    text = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1<prd>\\2\", text)\n",
        "    text = re.sub(r\"([A-Za-z])\\.([A-Za-z])\\.([A-Za-z])\", r\"\\1<prd>\\2<prd>\\3\", text)\n",
        "\n",
        "\n",
        "    # Step 4. Mark sentence boundaries\n",
        "    text = text.replace(\"! \", \"!<stop>\")\n",
        "    text = text.replace(\"? \", \"?<stop>\")\n",
        "    text = text.replace(\". \", \".<stop>\")\n",
        "    text = text.replace('...\"', '...\".<stop>')\n",
        "    text = text.replace('.\" ', '.\".<stop>')\n",
        "\n",
        "    # Step 5. Restore protected tokens\n",
        "    text = text.replace(\"<prd>\", \".\")\n",
        "    text = text.replace(\"<ellip>\", \"...\")\n",
        "\n",
        "    # Step 6. Split into sentences\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "\n",
        "    # Debug / output\n",
        "    for i, s in enumerate(sentences, 1):\n",
        "        print(f\"Sentence {i}: {s}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example\n",
        "#text = \"Dr. Smith lives in the U.S.A. He loves NLP... Do you? Visit www.ai.cn for more.\"\n",
        "text = \"\"\"AI is reshaping industries globally. Sr. Carlos Méndez works at x.ai. He says: \"NLP is tough...\" His team in U.S.A. develops models for multilingual texts. For example, their accuracy is 95.6%. However, challenges persist. Consider \"bank\"—is it a riverbank or a financial institution? In Beijing, **中国科学院** tackles similar issues. Their website, www.ai.cn, documents progress. Dr. Li Wei notes: \"We need better algorithms.\" Meanwhile, en Madrid, Srta. Ana Ruiz leads a project on Español NLP. Her team's budget? $2.3M. They collaborate with ETH Zürich, per x.ai's guidelines. Isn't that cool? Ambiguities like \"lead\" (metal or guide) complicate things... Moreover, slang like \"lol\" confuses models. In **中国**, Weibo posts use \"666\" for praise. Such nuances demand advanced processing. xAI's Grok aims to solve this. Visit x.ai for details. Their team hopes to launch by 2026. Processing costs are high—3.14x last year's budget. AI's future is bright!\"\"\"\n",
        "\n",
        "sentence_segmentation(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1twGMwN6HHX",
        "outputId": "57e77958-a0ea-4833-e775-62ebe65362f4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: AI is reshaping industries globally.\n",
            "Sentence 2: Sr. Carlos Méndez works at x.ai.\n",
            "Sentence 3: He says: \"NLP is tough...\".\n",
            "Sentence 4: His team in U.S.A. develops models for multilingual texts.\n",
            "Sentence 5: For example, their accuracy is 95.6%.\n",
            "Sentence 6: However, challenges persist.\n",
            "Sentence 7: Consider \"bank\"—is it a riverbank or a financial institution?\n",
            "Sentence 8: In Beijing, **中国科学院** tackles similar issues.\n",
            "Sentence 9: Their website, www.ai.cn, documents progress.\n",
            "Sentence 10: Dr. Li Wei notes: \"We need better algorithms.\".\n",
            "Sentence 11: Meanwhile, en Madrid, Srta. Ana Ruiz leads a project on Español NLP.\n",
            "Sentence 12: Her team's budget?\n",
            "Sentence 13: $2.3M.\n",
            "Sentence 14: They collaborate with ETH Zürich, per x.ai's guidelines.\n",
            "Sentence 15: Isn't that cool?\n",
            "Sentence 16: Ambiguities like \"lead\" (metal or guide) complicate things...\n",
            "Sentence 17: Moreover, slang like \"lol\" confuses models.\n",
            "Sentence 18: In **中国**, Weibo posts use \"666\" for praise.\n",
            "Sentence 19: Such nuances demand advanced processing.\n",
            "Sentence 20: xAI's Grok aims to solve this.\n",
            "Sentence 21: Visit x.ai for details.\n",
            "Sentence 22: Their team hopes to launch by 2026.\n",
            "Sentence 23: Processing costs are high—3.14x last year's budget.\n",
            "Sentence 24: AI's future is bright!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sentence Segmentation"
      ],
      "metadata": {
        "id": "SVe9ut7JFxic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhpfPQ_DFwsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimum Edit Distance\n",
        "with and without nltk"
      ],
      "metadata": {
        "id": "sNmEyou7Ewyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_distance(str1, str2):\n",
        "    m, n = len(str1), len(str2)\n",
        "\n",
        "    # Create a DP table\n",
        "    dp = [[0] * (n + 1) for i in range(m + 1)]\n",
        "\n",
        "    # Fill the base cases\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i   # cost of deleting all characters\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j   # cost of inserting all characters\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i-1] == str2[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1]  # no operation needed\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(\n",
        "                    dp[i-1][j],    # deletion\n",
        "                    dp[i][j-1],    # insertion\n",
        "                    dp[i-1][j-1]   # substitution\n",
        "                )\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "print(edit_distance(\"kitten\", \"sitting\"))  # Output: 3\n",
        "print(edit_distance(\"sunday\", \"saturday\"))  # Output: 3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_iun8rD3WHX",
        "outputId": "a0d3cc25-3fa8-4750-cb48-3592c86a0d9c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "# Example words\n",
        "word1 = \"kitten\"\n",
        "word2 = \"sitting\"\n",
        "\n",
        "# Compute edit distance\n",
        "distance = edit_distance(word1, word2)\n",
        "print(distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri37RZWdGqNV",
        "outputId": "a3e888eb-f367-4968-beee-02c1219d5533"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Morphological Analysis"
      ],
      "metadata": {
        "id": "g8LKoNTJGLBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "prefixes = [\"un\", \"re\", \"dis\", \"in\", \"mis\", \"pre\", \"non\", \"over\", \"de\", \"anti\"]\n",
        "suffixes = [\"ing\", \"ed\", \"ness\", \"ment\", \"ify\", \"s\", \"ly\", \"al\", \"er\", \"ion\", \"able\"]\n",
        "\n",
        "words = [\n",
        "    \"unhappiness\", \"replayed\", \"dislike\", \"running\", \"government\", \"inactive\",\n",
        "    \"simplify\", \"cats\", \"misleading\", \"previewed\", \"nonfiction\", \"overcooked\",\n",
        "    \"deactivated\", \"antivirus\", \"reorganize\", \"unfriendly\", \"walked\", \"happiness\",\n",
        "    \"employment\", \"readable\", \"national\", \"teachers\", \"disapproval\", \"prepayment\",\n",
        "    \"informer\"\n",
        "]\n",
        "\n",
        "def analyze_word(word):\n",
        "    root = word\n",
        "    pre = \"\"\n",
        "    suf = \"\"\n",
        "\n",
        "    for p in prefixes:\n",
        "        if word.startswith(p):\n",
        "            pre = p\n",
        "            root = root[len(p):]\n",
        "            break\n",
        "\n",
        "    for s in suffixes:\n",
        "        if root.endswith(s):\n",
        "            suf = s\n",
        "            root = root[: -len(s)]\n",
        "            break\n",
        "\n",
        "    return (pre, root, suf)\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w} -> {analyze_word(w)}\")"
      ],
      "metadata": {
        "id": "biZyaug3EvO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inflectional and derivational"
      ],
      "metadata": {
        "id": "1sSh59szF6nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predefined Affix Lists\n",
        "PREFIXES = sorted([\n",
        "    \"un\", \"re\", \"in\", \"im\", \"dis\", \"mis\", \"pre\", \"non\",\n",
        "    \"over\", \"under\", \"inter\", \"trans\", \"sub\", \"super\", \"anti\", \"auto\"\n",
        "], key=len, reverse=True)\n",
        "\n",
        "INFLECTIONAL_SUFFIXES = [\"s\", \"es\", \"ed\", \"ing\", \"er\", \"est\"]\n",
        "DERIVATIONAL_SUFFIXES = [\n",
        "    \"ness\", \"less\", \"ful\", \"able\", \"ible\", \"ment\", \"tion\", \"sion\",\n",
        "    \"al\", \"ly\", \"ous\", \"ive\", \"ize\", \"ship\", \"hood\", \"ist\", \"ism\", \"y\", \"er\"\n",
        "]\n",
        "SUFFIXES = sorted(INFLECTIONAL_SUFFIXES + DERIVATIONAL_SUFFIXES, key=len, reverse=True)\n",
        "\n",
        "def analyze_morphemes_simple(word):\n",
        "    original_word = word\n",
        "    prefixes = []\n",
        "    suffixes = []\n",
        "\n",
        "    # Remove prefixes iteratively\n",
        "    for p in PREFIXES:\n",
        "        if word.startswith(p) and len(word) > len(p) + 1:\n",
        "            prefixes.append(p)\n",
        "            word = word[len(p):]\n",
        "            break  # only take the first longest match\n",
        "\n",
        "    # Remove suffixes iteratively\n",
        "    while True:\n",
        "        matched = False\n",
        "        for s in SUFFIXES:\n",
        "            if word.endswith(s) and len(word) > len(s) + 1:\n",
        "                suffixes.append(s)\n",
        "                word = word[:-len(s)]\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            break\n",
        "\n",
        "    # Determine type\n",
        "    affix_types = set()\n",
        "    for s in suffixes:\n",
        "        if s in INFLECTIONAL_SUFFIXES:\n",
        "            affix_types.add(\"inflectional\")\n",
        "        if s in DERIVATIONAL_SUFFIXES:\n",
        "            affix_types.add(\"derivational\")\n",
        "    for p in prefixes:\n",
        "        affix_types.add(\"derivational\")\n",
        "\n",
        "    if not affix_types:\n",
        "        affix_type = \"none\"\n",
        "    elif len(affix_types) == 1:\n",
        "        affix_type = affix_types.pop()\n",
        "    else:\n",
        "        affix_type = \"both\"\n",
        "\n",
        "    return {\n",
        "        \"root\": word,\n",
        "        \"prefixes\": prefixes,\n",
        "        \"suffixes\": suffixes,\n",
        "        \"type\": affix_type\n",
        "    }\n",
        "\n",
        "words = [\"unhappiness\", \"running\", \"bookshelf\", \"uncharacteristically\",\n",
        "  \"reestablishing\", \"firefighter\"]\n",
        "for w in words:\n",
        "  print(w, \"->\", analyze_morphemes_simple(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noA3_XMeFV5P",
        "outputId": "9d16303d-42aa-4293-f160-61eb7e9f547f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unhappiness -> {'root': 'happi', 'prefixes': ['un'], 'suffixes': ['ness'], 'type': 'derivational'}\n",
            "running -> {'root': 'runn', 'prefixes': [], 'suffixes': ['ing'], 'type': 'inflectional'}\n",
            "bookshelf -> {'root': 'bookshelf', 'prefixes': [], 'suffixes': [], 'type': 'none'}\n",
            "uncharacteristically -> {'root': 'characteristic', 'prefixes': ['un'], 'suffixes': ['ly', 'al'], 'type': 'derivational'}\n",
            "reestablishing -> {'root': 'establish', 'prefixes': ['re'], 'suffixes': ['ing'], 'type': 'both'}\n",
            "firefighter -> {'root': 'firefight', 'prefixes': [], 'suffixes': ['er'], 'type': 'both'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpora Analysis"
      ],
      "metadata": {
        "id": "TOQj-lO-FT9o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGDs6hziFhU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell Checker"
      ],
      "metadata": {
        "id": "PaWVyAqyFh0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def simple_spell_checker(text, dictionary):\n",
        "    tokens = word_tokenize(text)\n",
        "    misspelled = []\n",
        "\n",
        "    # find words not in dictionary\n",
        "    for word in tokens:\n",
        "        if word.isalpha() and word.lower() not in dictionary:\n",
        "            misspelled.append(word)\n",
        "\n",
        "    # give suggestions\n",
        "    for word in set(misspelled):\n",
        "        distances = [(w, edit_distance(word.lower(), w)) for w in dictionary]\n",
        "        distances.sort(key=lambda x: x[1])   # sort by edit distance\n",
        "        suggestions = [w for w, d in distances[:3]]  # top 3\n",
        "        print(f\"{word} -> {suggestions}\")\n",
        "\n",
        "# Example\n",
        "dictionary = {\"artificial\", \"intelligence\", \"companies\", \"develops\", \"precision\",\n",
        "              \"researchers\", \"optimistic\", \"communication\", \"accuracy\"}\n",
        "\n",
        "text = \"Artifical inteligence is powerfull but it requir precisin.\"\n",
        "simple_spell_checker(text, dictionary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZHEdiWYpLH8",
        "outputId": "a457c660-3fcb-4e7d-b958-fdec329face6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "powerfull -> ['develops', 'companies', 'accuracy']\n",
            "Artifical -> ['artificial', 'accuracy', 'precision']\n",
            "but -> ['accuracy', 'develops', 'artificial']\n",
            "is -> ['develops', 'companies', 'precision']\n",
            "inteligence -> ['intelligence', 'optimistic', 'artificial']\n",
            "precisin -> ['precision', 'optimistic', 'artificial']\n",
            "it -> ['develops', 'companies', 'accuracy']\n",
            "requir -> ['precision', 'develops', 'accuracy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "jakhibJVpzal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\") # Add this line to download the missing resource\n",
        "\n",
        "\n",
        "text = \"The cat chases the dog. Dogs are friendly. Children play with hats.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Count singular/plural nouns\n",
        "counts = Counter()\n",
        "for word, tag in tagged:\n",
        "    if tag in (\"NN\", \"NNS\"):\n",
        "        lemma = lemmatizer.lemmatize(word.lower(), \"n\")\n",
        "        form = \"singular\" if tag == \"NN\" else \"plural\"\n",
        "        counts[(lemma, form)] += 1\n",
        "\n",
        "# Print\n",
        "for (lemma, form), c in counts.items():\n",
        "    print(f\"{lemma},{form},{c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEQKTwQppLlD",
        "outputId": "e083f8eb-309a-458c-8ad5-d0acd066018b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat,singular,1\n",
            "dog,singular,1\n",
            "dog,plural,1\n",
            "play,singular,1\n",
            "hat,plural,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "text = \"\"\"\n",
        "You must go to the store.\n",
        "The project must meet deadlines.\n",
        "She must be happy.\n",
        "We must not delay.\n",
        "This must end now.\n",
        "\"\"\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for sent in sentences:\n",
        "    tagged = pos_tag(word_tokenize(sent))\n",
        "    for i, (word, tag) in enumerate(tagged):\n",
        "        if word.lower() == \"must\" and i+1 < len(tagged):\n",
        "            print(f\"{word} + {tagged[i+1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ZZCC9lp1L0",
        "outputId": "0c715f1c-dee8-4b30-8be8-5ace4f900a63"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "must + ('go', 'VB')\n",
            "must + ('meet', 'VB')\n",
            "must + ('be', 'VB')\n",
            "must + ('not', 'RB')\n",
            "must + ('end', 'VB')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJb35qrWqAqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}